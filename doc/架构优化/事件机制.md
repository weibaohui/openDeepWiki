# 事件机制（实时状态与解耦通信）架构优化建议

## 1. 现状与问题定位

当前系统的“状态变化传播”主要依赖前端轮询与后端写死的状态更新逻辑：

- 前端通过定时轮询拉取仓库/任务/文档状态  
  - [RepoDetail.tsx](file:///Users/weibh/projects/go/openDeepWiki/frontend/src/pages/RepoDetail.tsx#L33-L37)：每 3 秒轮询一次仓库详情、任务列表、文档列表  
  - [Home.tsx](file:///Users/weibh/projects/go/openDeepWiki/frontend/src/pages/Home.tsx#L26-L30)：每 5 秒轮询一次仓库列表
- 后端状态更新散落在不同 goroutine 中，缺少统一“状态变化事件”的发布与订阅能力  
  - [RepositoryService.runTasksAsync](file:///Users/weibh/projects/go/openDeepWiki/backend/services/repository.go#L132-L159)：在单个 goroutine 串行跑任务，最后根据 failedCount 设置仓库状态  
  - [TaskService.Run](file:///Users/weibh/projects/go/openDeepWiki/backend/services/task.go#L39-L129)：在任务运行过程中更新 task 状态并写 DB，但不会对外广播变化

这会带来几个架构层面的长期问题：

1) 体验与资源浪费  
- 轮询在“无变化”时产生大量无效请求；变化发生时也只能在下一次轮询被动发现，实时性差。

2) 系统耦合与扩展性差  
- “状态变化 -> UI 刷新”被固定在“HTTP 拉取”模式，后续扩展（例如：实时日志流、增量进度、任务队列排队信息）会越来越难。

3) 任务/仓库状态的归因不清晰  
- 没有统一事件流，排障只能看散落日志与 DB 状态，缺乏“发生了什么”的可重放记录。

## 2. 目标架构（优先建议：后端事件总线 + SSE）

推荐落地路径（从低成本到可扩展）：

### 2.1 组件拆分

- **EventBus（进程内事件总线）**：负责发布/订阅事件（Task/Repo/Doc 等领域事件）
- **EventStream（SSE 事件流）**：为前端提供按 repo 维度的实时推送（EventSource）
- **Publisher（发布者）**：在服务层（RepositoryService/TaskService/DocumentService）产生日志/状态变化事件并发布
- **Consumer（消费者）**：前端 RepoDetail/Home 等页面订阅 SSE，收到事件后局部刷新或直接应用增量更新

为什么建议 SSE 而不是 WebSocket：

- SSE 对“服务器主动推送、客户端只读”的场景更简单稳定（事件序列 + 自动重连）
- gin/HTTP 基础设施更容易接入（对连接管理与鉴权也更直观）
- 业务上主要是状态更新与日志推送，不强依赖双向通信

## 3. 事件模型设计（建议）

### 3.1 事件协议（统一 envelope）

建议统一使用如下事件结构（字段可按实际裁剪）：

| 字段 | 说明 |
|---|---|
| type | 事件类型，例如 repo.status_changed / task.status_changed / doc.created |
| ts | 事件时间戳（毫秒） |
| repo_id | 关联仓库（可选） |
| task_id | 关联任务（可选） |
| payload | 事件内容（JSON 对象） |
| request_id | 关联一次操作链路（可选，便于排障） |

### 3.2 事件类型（最小可用集合）

| 事件类型 | 触发点 | payload 建议 |
|---|---|---|
| repo.status_changed | 仓库状态迁移（cloning/ready/analyzing/completed/error） | {from,to,error_msg?} |
| task.status_changed | 任务状态迁移（pending/running/completed/failed） | {from,to,error_msg?,started_at?,completed_at?} |
| doc.created | 文档生成成功 | {doc_id,title,filename} |
| task.progress | 任务内部阶段进度（静态分析/LLM/保存文档） | {phase,percent?,message?} |
| log.append | 任务日志追加（可选） | {level,message} |

注意：事件与 DB 状态并不冲突，事件用于实时传播与解耦；DB 仍是最终一致的事实源。

## 4. 与当前代码的映射（改造点）

### 4.1 后端发布事件（服务层）

在以下位置发布事件最合适：

- 仓库状态变化：  
  - clone 开始/成功/失败：[RepositoryService.cloneAndAnalyze](file:///Users/weibh/projects/go/openDeepWiki/backend/services/repository.go#L58-L82)
  - run-all 启动/结束：[RepositoryService.RunAllTasks](file:///Users/weibh/projects/go/openDeepWiki/backend/services/repository.go#L114-L130)、[RepositoryService.runTasksAsync](file:///Users/weibh/projects/go/openDeepWiki/backend/services/repository.go#L132-L159)
- 任务状态变化：  
  - pending -> running、running -> completed/failed：[TaskService.Run](file:///Users/weibh/projects/go/openDeepWiki/backend/services/task.go#L56-L129)
  - reset/force reset：[TaskService.Reset](file:///Users/weibh/projects/go/openDeepWiki/backend/services/task.go#L152-L166)、[TaskService.ForceReset](file:///Users/weibh/projects/go/openDeepWiki/backend/services/task.go#L169-L190)
- 文档生成成功：  
  - [DocumentService.Create](file:///Users/weibh/projects/go/openDeepWiki/backend/services/document.go#L29-L43)

### 4.2 后端事件流 API（SSE）

建议新增接口（示例）：

- `GET /api/repositories/:id/events`：返回 SSE 流，仅推送与该 repo 相关事件  

路由可在 [router.go](file:///Users/weibh/projects/go/openDeepWiki/backend/router/router.go#L30-L63) 中新增一个 `repos.GET("/:id/events", ...)`。

鉴权建议（强烈建议与配置管理一起做）：  
- 事件流也需要鉴权，否则相当于公开推送仓库运行信息。

### 4.3 前端接入

以仓库详情页为例：

- 将 [RepoDetail.tsx](file:///Users/weibh/projects/go/openDeepWiki/frontend/src/pages/RepoDetail.tsx#L33-L37) 的 setInterval 轮询替换为 EventSource  
- 收到 `task.status_changed/doc.created/repo.status_changed` 后：
  - 轻量方案：触发一次 fetchData（仍走 HTTP 拉取，但只在“有变化”时拉）
  - 进阶方案：直接把 payload 合并进本地 state（减少接口调用）

Home 列表页也可按 repo 级别订阅“仓库状态变化”事件，或保留低频轮询（例如 30s）作为兜底。

## 5. 内存消息机制的选型建议（从简到强）

### 5.1 进程内 Pub/Sub（最小落地）

适用于单实例部署，收益最大、成本最低：

- EventBus 使用 `map[repoID]map[subscriberID]chan Event` 管理订阅者
- 发布者向所有订阅者 fan-out
- SSE handler 每个连接绑定一个订阅，断开自动 unsubscribe

缺点：多实例部署时事件不共享。

### 5.2 进程外消息（后续演进）

当需要多实例/水平扩展时，可升级为：

- Redis Pub/Sub / Redis Streams
- NATS / Kafka（更重但更强）

演进策略：先把“事件协议 + 发布点”稳定下来，再替换总线实现即可。

## 6. 推荐的分阶段落地计划（不依赖大重构）

### 阶段 A：先把轮询变成“事件触发拉取”

1) 后端引入 EventBus（内存）并新增 SSE 接口  
2) 服务层在关键状态变更处发布事件（仓库/任务/文档）  
3) 前端使用 EventSource，收到事件后触发 fetchData（替代 setInterval）

验收标准：

- RepoDetail 页面不再 3 秒轮询
- 任务状态变化在 1s 内反映到 UI
- 后端请求量明显下降（无变化时接近 0）

### 阶段 B：增量更新与实时日志

1) 增加 task.progress/log.append 事件  
2) 前端按事件 payload 增量更新 tasks/documents，减少全量拉取  
3) 增加 UI：任务运行阶段/日志面板

## 7. 与“仓库已完成状态”的关系（你提到的例子）

你观察到的点本质是“仓库状态应该由任务集合状态驱动”，而不是在某个固定位置写死：

- 当前实现是在 [runTasksAsync](file:///Users/weibh/projects/go/openDeepWiki/backend/services/repository.go#L132-L159) 末尾通过 failedCount 推断仓库状态  
- 建议改造成：每次 `task.status_changed` 事件发生后，由一个聚合器（RepoStatusAggregator）计算仓库应处于的状态并发布 `repo.status_changed`

这样：

- 单任务运行（不是 run-all）也能让仓库状态合理变化
- 前端可以只订阅 repo.status_changed 来更新全局状态
- 将来增加新任务类型、并行执行、人工跳过等，都不需要到处改写“写死逻辑”

